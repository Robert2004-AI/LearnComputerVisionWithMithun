{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Robert2004-AI/LearnComputerVisionWithMithun/blob/main/ADL_Assigment01_Task3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Group No - 44\n",
        "\n",
        "**Group Member Names:**\n",
        "\n",
        "SAKTHI R (2023aa05940)\n",
        "\n",
        "ROBERTSEKAR R (2023aa05823)\n",
        "\n",
        "RAVISHANKAR R (2023aa05171\n",
        "\n",
        "NIMBALKAR PRITEESH DADASAHEB (2023aa05950)"
      ],
      "metadata": {
        "id": "Mk0scK5vLm2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3:\n",
        "\n",
        "Train an appropriate deep convolutional autoencoder with same dimension of latent space. Calculate the reconstruction error fand compare that with a single hidden layer autoencoder (with sigmoid activation at the autoencoder and linear at the decoder) for the test dataset. What will be the reconstruction error if the hidden nodes are distributed equally (approximately) among 3 hidden layers in a new 3 hidden layer autoencoder with sigmoid activation at the autoencoder and linear at the decoder final layer?"
      ],
      "metadata": {
        "id": "7fCZnyppMoGu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m9hW9B2fJwh5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.datasets import fetch_openml\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# ---- Load MNIST Dataset ----\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    Load the MNIST dataset from OpenML, normalize the pixel values to [0, 1],\n",
        "    and split into training and testing sets.\n",
        "    \"\"\"\n",
        "    mnist = fetch_openml('mnist_784', version=1)  # Fetch MNIST dataset\n",
        "    X = mnist.data / 255.0  # Normalize pixel values to [0, 1]\n",
        "    y = mnist.target.astype(int)  # Convert target labels to integers\n",
        "    # Split the data into training (70%) and testing (30%) sets\n",
        "    return train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep Convolutional Autoencoder\n",
        "latent_dim = 50  # Set the latent space dimensionality\n",
        "\n",
        "def build_autoencoder(input_dim, latent_dim):\n",
        "    \"\"\"\n",
        "    Build a single-layer linear autoencoder model.\n",
        "\n",
        "    Parameters:\n",
        "        input_dim (int): Dimensionality of the input data.\n",
        "        latent_dim (int): Dimensionality of the latent (compressed) space.\n",
        "\n",
        "    Returns:\n",
        "        autoencoder (Model): Compiled autoencoder model.\n",
        "    \"\"\"\n",
        "    # Define the input layer with the same shape as the input data\n",
        "    input_layer = layers.Input(shape=(input_dim,))\n",
        "\n",
        "    # Encoder: Compress input to latent space of size 'latent_dim' with linear activation\n",
        "    encoded = layers.Dense(latent_dim, activation='linear')(input_layer)\n",
        "\n",
        "    # Decoder: Reconstruct input from the latent space with linear activation\n",
        "    decoded = layers.Dense(input_dim, activation='linear')(encoded)\n",
        "\n",
        "    # Create the autoencoder model combining encoder and decoder\n",
        "    autoencoder = models.Model(input_layer, decoded)\n",
        "\n",
        "    # Compile the model with Adam optimizer and Mean Squared Error (MSE) loss\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "    return autoencoder\n",
        "\n",
        "def build_deep_autoencoder(input_shape, latent_dim):\n",
        "    \"\"\"\n",
        "    Build a deep convolutional autoencoder model.\n",
        "\n",
        "    Parameters:\n",
        "        input_shape (tuple): Shape of the input data (e.g., (28, 28, 1) for MNIST images).\n",
        "        latent_dim (int): Dimensionality of the latent (compressed) space.\n",
        "\n",
        "    Returns:\n",
        "        autoencoder (Model): Compiled autoencoder model.\n",
        "    \"\"\"\n",
        "    # Input layer for image data with specified shape\n",
        "    input_layer = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Encoder: First convolutional layer with 32 filters, 3x3 kernel, ReLU activation, and same padding\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
        "    # Downsampling using max pooling to reduce spatial dimensions\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    # Encoder: Second convolutional layer with 64 filters, 3x3 kernel, ReLU activation, and same padding\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    # Further downsampling using max pooling\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    # Flatten the feature maps into a 1D vector for the fully connected layer\n",
        "    x = layers.Flatten()(x)\n",
        "    # Fully connected layer to compress data into the latent space\n",
        "    encoded = layers.Dense(latent_dim, activation='relu')(x)\n",
        "    encoded.name = \"dense_2\"  # Naming the layer for easier access\n",
        "\n",
        "    # Decoder: Expand latent representation back to original input shape\n",
        "    # Fully connected layer to reconstruct the flattened input\n",
        "    decoder_input = layers.Reshape((latent_dim,))(encoded)\n",
        "    decoder = layers.Dense(np.prod(input_shape), activation='sigmoid')(decoder_input)\n",
        "    # Reshape the flattened output back into the original input dimensions\n",
        "    decoded = layers.Reshape(input_shape)(decoder)\n",
        "\n",
        "    # Create the autoencoder model combining encoder and decoder\n",
        "    autoencoder = models.Model(input_layer, decoded)\n",
        "\n",
        "    # Compile the model with Adam optimizer and Mean Squared Error (MSE) loss\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "    return autoencoder\n",
        "\n",
        "# ---- Main Execution ----\n",
        "\n",
        "# Reshape training and test data into 4D format for convolutional layers\n",
        "# Each image is reshaped to (28, 28, 1) to represent height, width, and channels\n",
        "X_train_reshaped = X_train.values.reshape(-1, 28, 28, 1)\n",
        "X_test_reshaped = X_test.values.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Build the deep convolutional autoencoder\n",
        "deep_autoencoder = build_deep_autoencoder((28, 28, 1), latent_dim)\n",
        "\n",
        "# Train the autoencoder using the reshaped training data\n",
        "# The goal is to reconstruct the input images while learning a compressed latent representation\n",
        "print(\"Training Deep Convolutional Autoencoder...\")\n",
        "deep_autoencoder.fit(X_train_reshaped, X_train_reshaped, epochs=10, batch_size=256)\n",
        "\n",
        "# Evaluate the reconstruction error on the test set\n",
        "# This quantifies how well the autoencoder reconstructs unseen data\n",
        "reconstruction_error_deep = deep_autoencoder.evaluate(X_test_reshaped, X_test_reshaped, verbose=0)\n",
        "print(f\"Reconstruction Error (Deep Autoencoder): {reconstruction_error_deep:.4f}\")\n",
        "\n",
        "\n",
        "# Train a single hidden layer autoencoder for comparison\n",
        "print(\"Training Single Hidden Layer Autoencoder\")\n",
        "# Build a simple autoencoder with a single dense layer\n",
        "autoencoder = build_autoencoder(X_train.shape[1], latent_dim)\n",
        "# Train the simple autoencoder\n",
        "autoencoder.fit(X_train, X_train, epochs=10, batch_size=256, shuffle=True)\n",
        "\n",
        "# Evaluate the reconstruction error for the single-layer autoencoder\n",
        "reconstruction_error_single = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
        "print(f\"Reconstruction Error (Single Hidden Layer): {reconstruction_error_single:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVdXFS43UQfV",
        "outputId": "6174d37d-cf35-4eff-ceaa-c7236e0bf99a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Deep Convolutional Autoencoder...\n",
            "Epoch 1/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 0.1045\n",
            "Epoch 2/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0308\n",
            "Epoch 3/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0207\n",
            "Epoch 4/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0167\n",
            "Epoch 5/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0144\n",
            "Epoch 6/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0129\n",
            "Epoch 7/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0119\n",
            "Epoch 8/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0113\n",
            "Epoch 9/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0108\n",
            "Epoch 10/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0104\n",
            "Reconstruction Error (Deep Autoencoder): 0.0103\n",
            "Training Single Hidden Layer Autoencoder\n",
            "Epoch 1/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0655\n",
            "Epoch 2/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0206\n",
            "Epoch 3/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0148\n",
            "Epoch 4/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0129\n",
            "Epoch 5/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0123\n",
            "Epoch 6/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0121\n",
            "Epoch 7/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0120\n",
            "Epoch 8/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0119\n",
            "Epoch 9/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0119\n",
            "Epoch 10/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0119\n",
            "Reconstruction Error (Single Hidden Layer): 0.0119\n"
          ]
        }
      ]
    }
  ]
}